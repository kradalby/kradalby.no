Title: TDT4186 Operating system 
Date: 2015-05-13 10:00
Modified: 
Summary: My notes for TDT4186

[TOC]

# Intro
These are my notes for TDT4186 . These are my own understandings of the subject and it may not be completely correct or accurate. It is also written in slightly below average Norwegian English.

## Why do I post them here?
There is a couple of reasons. The first is for convenience. I like to write Markdown as it is very easy to structure text easily and it does not take any focus away from the text, it also has great readability both uncompiled and compiled. It is also very practical to have this available on the web as i like to read my notes on my iPad, and it is faster to access here. The last reason is that someone may find them useful.

# Chapter 1 - Computer Overview

## Instruction Execution
When a Instruction is to be Executed, the instruction cycle is as follows:

    :::
    Start -> Fetch Next -> Execute -> Halt

## Interrupts
Interrupt is a mechanism which is used to tell a CPU that the task you are currently doing, must be paused and put back to the queue so the next process can get CPU time.

**Program:** Fault in the program, such as devision by zero and exceptions.
**Timer:** Generated by a timer within the processor.
**I/O:** Generated by an I/O controller to signal completion of task or error.
**Hardware fail:** Generated by hardware failure, such as powerfailure or memory parity error.

In the instruction execution cycle, checking for interrupts are added at the end after the Execute step. The interrupt is checked between every instruction.

## The Memory Hierarchy
The morst important parts of the memory hierarchy is the inboard memory and outboard storage.

**Inboard memory:** Registers, cache, main memory
**Outboard storage:** SSD, HDD, Optical Drive
**Offline storage:** Magnetic tape

The top of the hierarchy is the fastest and it is typically more expensive per byte. The closer the memory is to the CPU the faster, sometimes, like registers and cache, its onboard the CPU.

## Cache Memory
Cache is a memory storage inside or very close to the CPU. It is very fast, and usually operates on the same bandwith as the CPU. Typically the needed information is transfered from the main memory to the cache and then to the CPU/registers. If the system has multiple levels of cache, it will go the entier latter from slowest to fastes before hiting the CPU.

The CPU and sometimes software, has implemented algorithms to try to predict the next parts of needed information.

## Direct Memory Access
DMA is a technique where the CPU uses the DMA module on the system bus is used to move data from one point to the other. It does not send any interrupt signal until the job is finished or fails.

The CPU tells the DMA module the following info:

* Read or write
* Address of the I/O device involved
* Starting location in memory to read/write
* The number of words to be written

### Programmed I/O
Programmed I/O does not interrupt the processor when a instruction related to I/O is executed. It sets the I/O status register bits, but does not talk to the processor.

### Interrupte-driven I/O
Will make the I/O module work on, and then put the process into the blocking queue. The CPU will then keep on doing something productive. The I/O module will then interrupt the processor when the data is ready.

Interrupted I/O is more efficient than Programmed I/O, but does require the CPU to transfer data between the memomry and the I/O module.




# Chapter 2 - OS Overview

## Virtual Machines

# Chapter 3 - Processes

## What is a Process?
A program can be defined as:

* A program in execution
* An instance of a program running on a computer
* The entity that can be executed on a processor
* A unit of acitivty characterized by the execution of a sequence of instructions, a current state, and an associated set of system resources.

Two essential elements of a process are program code and a set of data.

While a process is executed, it includes the following:

**Identifier:** Unique identifier

**State:** The state, if executing, its in the running state.

**Priority:** Priority level relative to other processes.

**Program counter:** The address of the next instruction.

**Memory pointers:** Pointers to the code, assosiacted data and shared blocks.

**Context data:** Data present in the registers while executing.

**I/O status information:** Information about used devices and files, outstanding I/O requests.

**Accounting information:** Information about used time, clock time and time limits.



## Process States
A process can be in several different states, mostly five different states are used:

**Running:** The process is being executed.

**Ready:** A process that is ready to be executed.

**Blocked:** A process that cannot execute until some event occurs, e.g. completion of I/O.

**New:** A newly created process that are not in the queue, probably not loaded into memory.

**Exit:** A process that is not in the pool, either halted or aborted.

## Process Description

## Process Control

### Process control block
A data structure in the operating system kernel containing the information needed to manage a particular process. 

Usually contains:

* Process identification data
* Processor state data
* Process control data

From the earlier list of information regarding processes, everything except Identifier and State is control data.

# Chapter 4 - Threads

![Single thread process vs Multi thread process](https://kradalby.no/ss/20150515170650.png)

## Process vs Threads
Benefits of a thread:

* Faster creation
* Faster termination
* Faster switching between two threads
* Faster communication between two threads (no need to involve the kernel)

## Types of Threads
There are two main categories of thread implementation; the user-level threads and the kernel-level threads. 

### User-level threads
All work done related to the thread, the management and the excistence is not known to the kernel. The managment is done purly by the application running the thread. User threads are usually handled via libraries. 

Advantages:

* Thread switching does not require kernel mode
* Scheduling can be application specific
* ULTs can run on any OS

Disadvantages:

* OS system calls are typically blocking, the one thread blocks the entire process
* Pure ULT cannot take advantage of multiprocessing, one process one core.

#### Jacketing
Technique use to convert blocking system calls into nonblocking system call. A thread uses application-level I/O jacket routine to check if the I/O device is busy. 

### Kernel-level threads
All the thread management is done by the kernel. The main disadvantage of using a KLT is that a mode switch to give the kernel control is needed.

# Chapter 5 - Concurrency: Mutex and sync

## Principles of Concurrency

## Mutual Exclusion: Hardware
On uniprocessor system, the simplest solution to mutual exclusion is to disable interrupts in the CPU when a process critical section is running. This way, no other process can take over the CPU and modify the data. This solution however, is not optimal as jobs with long critical parts will choke the system. Also, if the job fails during the critical system, the CPU will not be given to another process, halting the whole system.

## Semaphores
A semaphore can be used to control access to a unit of particular resources. Semaphores which allows an arbitrary resource count are called counting semaphores and semaphores which only allows locked/unlocked or 0 and 1 is called binary semaphores.

## Monitors
Monitors or Synchronization(Java) is a construct that allows threads to have both mutual exclusion and the ability to wait/block for a certion condition to become true.

Monitors also has mechanism for signalling other threads that their condition has been met.

Monitors provide a mechanism for threads to temporarily give up exclusive access in order to wait for some condition to be met. 

## Message Passing
Is basically communication between processes. Can be used in building distributed systems, or inside a multiprogrammed system. 

It is important to consider what do to in two cases when handling message pasing between processes.

When a message is sent, should the process that has sent the message block until the message is received, or should it continue exectuing.

The same should be considered when receiving a message, if the receive functionality is executed, should it block/wait for it to receive anything or should it try to receive and then move on and continue executing.

**Blocking send, blocking receive:** Tight synchronization between processes.

**Nonblocking send, blocking receive:** Probably most useful. An example is a process that exists to provide information to other processes.

**Nonblocking send, nonblocking receive:** No waiting.

### Addressing
When sending a message one can have direct addressing, this is when the message is directly sent to one process.

One can also have indirect addressing, where the message is put into a shared data structure consisting of queues. The queue is generaly referred to as mailboxes.

Indirect addressing can be used to achieve one to one, one to many and many to many.

# Chapter 6 - Concurrency: Deadlock and Starvation

## Deadlock
Deadlock can be defined as the permanent blocking of a set of processes that either compete for system resource or comunicate with each other.

Typically processes are deadlocked when each process is blocked and waiting for one of the deadlocked processes to trigger an event or release a resource. The deadlock is permanent as nothing is going to be released since everybody waits for the next one.

### Conditions
To have a possibility of Deadlock, the first three conditions below must be met, if the forth is also met, the deadlock exist.

**Mutual exclusion:** Only one process may use a resource at a time.

**Hold and wait:** A process may hold the allocated resource while awaiting assignment of other resources.

**No preemption:** No resource can be forcibly removed from a process holding it.

**Circular wait:** A closed chain of processes exists, such that each process holds at least one resource needed by the next process in the chain.

### Prevention
Deadlock prevention is based on the idea of designing the system in a way that prevents one of the deadlock conditions to become true. There is two approaches, first, prevent one of the three first conditions or second, prevent circular wait.

**Mutual exclusion:** Cannot typically be disallowed.

**Hold and wait:** Can be prevented by requiring a process to request all resources at one time, and blocking the process until all requests has been granted simultaneously. 
This method is inefficient in two ways, first, waiting for all resources can block the process in a long time, and second it may not know all the required resources upon execution.

**No preemtion:** Can be prevented in several ways. First, if a process holding a resource is denied a further request, that process must release its original resources and, if necessary, request them again later.
Alternaltively, if a process requests a resource that is currently held by another process, the OS may preempt the second process and require it to release its resources. This can only prevent deadlock if no two processes has the same priority.

**Cirular wait:** Can be prevented by defining a linear ordering of resource types. If a process has been allocated resources of type R, then it may subsequently request only those resources of types following R in the ordering.

### Avoidance


### Detection

# Chapter 7 - Memory Management

## Memory Partitioning

## Paging

## Segmentation

# Chapter 8 - Virtual Memory

## Hardware and Control Structures

## Operating System Software

# Chapter 9 - Uniprocessor Scheduling

## Types of Processor Scheduling

## Scheduling Algorithms

### FCFS - First-Come, First-Served
FIFO queue where the first job will get the first time.
Can be very bad if short jobs comes after the long jobs.

### Round Robin
Every process will take turn for a time quantum. It is a fair algorithm, but not necessary the best.
Bad when all jobs are same length.

### SJF - Shortest-Job first
The CPU will try to run through the short jobs first. Big effect on short jobs, smaller effect on long. Better average response time.

One of the problems is to predict which jobs are the shortes. The possibility of starvation can also occour if the system always gets shorter jobs than a waiting long job.

#### Preemtive
If a new job arrives with less remaining time than the current job, swap them.
Also called Shortest-Remaining-Time-First(SRTF). Very good average response time. Optimal.

#### Non-preemtive
Once a job starts executing, it runs to completion

# Chapter 10 - Multiprocessor and RT Scheduling

## Multiprocessor Scheduling

## Real-Time Scheduling

# Chapter 11 - I/O Management and Disk Scheduling

## Devices

## Organization

## OS Design Issues

## Disk Scheduling

## RAID

## Disk Cache

# Chapter 12 - File Management

## File Organization and Access

## B-Trees

## File Directories

## File Sharing

## Record Blocking

## Secondary Storage Management

## File System Security

# Appendix A - Topics in Concurrency
